# -*- coding: utf-8 -*-
"""prediction_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lEyGra1_LdbJgiXA3zaKNMDLxeT0Wzml
"""

import pandas as pd
import numpy as np
import joblib
import os
import re
from sklearn.preprocessing import LabelEncoder, StandardScaler
from google.colab import drive

# Step 1: Mount Google Drive
drive.mount('/content/drive')

# Step 2: Define Drive Paths
# If you wanna try to use these script, copy the right path of your drive
drive_base_path = "/content/drive/MyDrive/Colab Notebooks/Dicoding Penerapan Data Science/Submission"
model_path = os.path.join(drive_base_path, "kmeans_clustering_model.joblib")
data_path = os.path.join(drive_base_path, "df_final.csv")

# -------------------------- Function Definitions -------------------------- #

def load_model(model_path):
    try:
        if not os.path.exists(model_path):
            print(f"Error: Model file not found at {model_path}")
            return None
        model = joblib.load(model_path)
        print(f"Model loaded successfully from: {model_path}")
        return model
    except Exception as e:
        print(f"Error loading model: {e}")
        return None

def format_column_names(df):
    df.columns = [re.sub(r'\s+', '_', col.lower()) for col in df.columns]
    return df

def encode_categorical_columns(df):
    df_encoded = df.copy()
    categorical_cols = df_encoded.select_dtypes(include=['object']).columns
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col])
        label_encoders[col] = le
        print(f"Encoded {col}: {list(le.classes_)}")
    return df_encoded, label_encoders

def preprocess_data(data_path):
    """
    Preprocess input data for clustering prediction.

    Args:
        data_path (str): Path to the CSV file containing data

    Returns:
        Tuple: (processed DataFrame ready for prediction, fully encoded DataFrame)
    """
    try:
        # Load CSV
        if not os.path.exists(data_path):
            print(f"Error: Data file not found at {data_path}")
            return None, None

        df = pd.read_csv(data_path)
        print(f"✅ Data loaded from: {data_path}, shape: {df.shape}")

        # Step 1: Standardize column names to snake_case
        df = format_column_names(df)
        print("✅ Column names formatted to snake_case.")

        # Step 2: Encode categorical columns
        df_encoded, label_encoders = encode_categorical_columns(df)

        # Step 3: Define expected features based on training
        expected_columns = [
            'business_travel', 'department', 'education_field', 'gender', 'job_role',
            'marital_status', 'over_time', 'transform_age', 'transform_daily_rate',
            'transform_distance_from_home', 'transform_education', 'transform_environment_satisfaction',
            'transform_hourly_rate', 'transform_job_involvement', 'transform_job_level',
            'transform_job_satisfaction', 'transform_monthly_income', 'transform_monthly_rate',
            'transform_num_companies_worked', 'transform_percent_salary_hike',
            'transform_performance_rating', 'transform_relationship_satisfaction',
            'transform_stock_option_level', 'transform_total_working_years',
            'transform_training_times_last_year', 'transform_work_life_balance',
            'transform_years_at_company', 'transform_years_in_current_role',
            'transform_years_since_last_promotion', 'transform_years_with_curr_manager'
        ]

        # Step 4: Create missing transform columns if original ones exist
        missing_cols = [col for col in expected_columns if col not in df_encoded.columns]
        transform_cols = [col for col in missing_cols if col.startswith('transform_')]
        original_cols = [col.replace('transform_', '') for col in transform_cols]

        for orig_col, trans_col in zip(original_cols, transform_cols):
            if orig_col in df_encoded.columns:
                scaler = StandardScaler()
                df_encoded[trans_col] = scaler.fit_transform(df_encoded[[orig_col]])
                print(f"✅ Created {trans_col} from {orig_col}")

        # Step 5: Fill any remaining missing expected columns with 0
        for col in expected_columns:
            if col not in df_encoded.columns:
                df_encoded[col] = 0
                print(f"⚠️ Column {col} missing — filled with 0 (placeholder)")

        # Step 6: Keep only expected columns
        df_final = df_encoded[expected_columns].copy()
        print(f"✅ Final preprocessed data shape: {df_final.shape}")

        return df_final, df_encoded

    except Exception as e:
        print(f"❌ Error during preprocessing: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def predict_clusters(model, data, original_data=None):
    try:
        clusters = model.predict(data)
        result_df = original_data if original_data is not None else data
        result_df = result_df.copy()
        result_df["employee_segment"] = clusters.astype(str)
        print(f"Cluster distribution: {pd.Series(clusters).value_counts().to_dict()}")
        return result_df
    except Exception as e:
        print(f"Error during prediction: {e}")
        return None

def main():
    model = load_model(model_path)
    if model is None:
        print("Model not loaded. Check file path.")
        return

    df_final, df_encoded = preprocess_data(data_path)
    if df_final is None:
        print("Data preprocessing failed.")
        return

    results = predict_clusters(model, df_final, df_encoded)
    if results is not None:
        print("\nSample Prediction Results:")
        print(results.head())
        # Save result to Drive
        output_path = os.path.join(drive_base_path, "employee_segments_results.csv")
        results.to_csv(output_path, index=False)
        print(f"\nPrediction results saved to: {output_path}")

# Run main function
main()